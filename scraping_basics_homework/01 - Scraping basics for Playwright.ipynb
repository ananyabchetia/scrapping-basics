{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping basics for Playwright\n",
    "\n",
    "This notebook is a combination of small scraping techniques along with how to use Playwright. Along with the class notes, the [scraping section](https://jonathansoma.com/everything/scraping/) on my Everything I Know site might be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import what you need to use Playwright, and start up a new browser to use for scraping. \n",
    "\n",
    "> If you end up opening a lot of Chromes/Chromiums, shutting down the Python kernel with the stop button is an easy way to make them go away! You'll have to re-run your notebook, but at least you won't have sixty icons in your dock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "playwright = await async_playwright().start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = await playwright.chromium.launch(headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = await browser.new_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/by-class.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/by-class.html' method='GET'>>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/by-class.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html><head><script>\\n    const html = `\\n<h1 class=\"title\">How to Scrape Things</h1>\\n<h3 class=\"subhead\">Probably using Playwright</h3>\\n<p class=\"byline\">By Jonathan Soma</p>\\n`\\n\\nsetTimeout(() => {\\n    console.log(html)\\n    document.querySelector(\\'body\\').innerHTML = html\\n}, 250)</script>\\n</head><body>\\n<h1 class=\"title\">How to Scrape Things</h1>\\n<h3 class=\"subhead\">Probably using Playwright</h3>\\n<p class=\"byline\">By Jonathan Soma</p>\\n</body></html>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <script>\n",
      "   const html = `\n",
      "<h1 class=\"title\">How to Scrape Things</h1>\n",
      "<h3 class=\"subhead\">Probably using Playwright</h3>\n",
      "<p class=\"byline\">By Jonathan Soma</p>\n",
      "`\n",
      "\n",
      "setTimeout(() => {\n",
      "    console.log(html)\n",
      "    document.querySelector('body').innerHTML = html\n",
      "}, 250)\n",
      "  </script>\n",
      " </head>\n",
      " <body>\n",
      "  <h1 class=\"title\">\n",
      "   How to Scrape Things\n",
      "  </h1>\n",
      "  <h3 class=\"subhead\">\n",
      "   Probably using Playwright\n",
      "  </h3>\n",
      "  <p class=\"byline\">\n",
      "   By Jonathan Soma\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Feed HTML to beautiful soup. \n",
    "\n",
    "soup_doc = BeautifulSoup(html)\n",
    "print(soup_doc.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping by class\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/by-class.html using their **class name**, printing out the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How to Scrape Things'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title=soup_doc.find(class_='title').text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subhead: Probably using Playwright\n"
     ]
    }
   ],
   "source": [
    "print('subhead:',soup_doc.find(class_='subhead').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "byline: By Jonathan Soma\n"
     ]
    }
   ],
   "source": [
    "print('byline:',soup_doc.find(class_='byline').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping using a single tag\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/by-list.html, creating a dictionary out of the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/by-list.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/by-list.html' method='GET'>>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(' http://jonathansoma.com/columbia/interactive-scrape/by-list.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<!DOCTYPE html><html><head><script>\\n    const html = `<p>How to Scrape Things</p>\\n<p>Probably using Playwright</p>\\n<p>By Jonathan Soma</p>\\n`\\n\\nsetTimeout(() => {\\n    console.log(html)\\n    document.querySelector('body').innerHTML = html\\n}, 250)</script>\\n</head><body><p>How to Scrape Things</p>\\n<p>Probably using Playwright</p>\\n<p>By Jonathan Soma</p>\\n</body></html>\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_2 = await page.content()\n",
    "html_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <script>\n",
      "   const html = `<p>How to Scrape Things</p>\n",
      "<p>Probably using Playwright</p>\n",
      "<p>By Jonathan Soma</p>\n",
      "`\n",
      "\n",
      "setTimeout(() => {\n",
      "    console.log(html)\n",
      "    document.querySelector('body').innerHTML = html\n",
      "}, 250)\n",
      "  </script>\n",
      " </head>\n",
      " <body>\n",
      "  <p>\n",
      "   How to Scrape Things\n",
      "  </p>\n",
      "  <p>\n",
      "   Probably using Playwright\n",
      "  </p>\n",
      "  <p>\n",
      "   By Jonathan Soma\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soup_doc = BeautifulSoup(html_2)\n",
    "print(soup_doc.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "<html><head><script>\n",
       "    const html = `<p>How to Scrape Things</p>\n",
       "<p>Probably using Playwright</p>\n",
       "<p>By Jonathan Soma</p>\n",
       "`\n",
       "\n",
       "setTimeout(() => {\n",
       "    console.log(html)\n",
       "    document.querySelector('body').innerHTML = html\n",
       "}, 250)</script>\n",
       "</head><body><p>How to Scrape Things</p>\n",
       "<p>Probably using Playwright</p>\n",
       "<p>By Jonathan Soma</p>\n",
       "</body></html>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<p>How to Scrape Things</p>,\n",
       " <p>Probably using Playwright</p>,\n",
       " <p>By Jonathan Soma</p>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_doc.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Waiting\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/by-tag-wait.html just like you above, but use  **wait_for** to wait for the text \"Everything has shown up\" to show up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = await browser.new_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await page.goto(\"http://jonathansoma.com/columbia/interactive-scrape/by-tag-wait.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = await page.content()\n",
    "soup_doc = BeautifulSoup(html, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await page.get_by_text(\"Everything has shown up\").wait_for()\n",
    "paras = soup_doc_three.find_all('p')\n",
    "\n",
    "for para in paras:\n",
    "    my_dict = {}\n",
    "    my_dict['title'] = paras[0].text\n",
    "    my_dict['subhead'] =paras[1].text\n",
    "    my_dict['byline'] = paras[2].text\n",
    "    my_dict['last_key'] =paras[3].text\n",
    "\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forms\n",
    "\n",
    "Display the content of the `h1` tag on http://jonathansoma.com/columbia/interactive-scrape/inputs.html. You'll need to follow the instructions to complete the form first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = await browser.new_page()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/inputs.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/inputs.html' method='GET'>>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/inputs.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html><head><script>\\n    const html = `<h1>You did it</h1>`\\n</script>\\n</head><body>\\n    <div id=\"things\"></div>\\n    <p>The secret is\\n    <select>\\n        <option selected=\"\">Closed</option>\\n        <option>Open</option>\\n    </select>\\n</p>\\n<p>\\n    <input type=\"text\" placeholder=\"write cat in here\" id=\"best-animal\">\\n</p>\\n<p>\\n    <input type=\"button\" id=\"submit\" value=\"Click me\">\\n</p>\\n    <script>\\n        document.querySelector(\"#submit\").addEventListener(\\'click\\', function() {\\n            if(document.querySelector(\\'#best-animal\\').value == \\'cat\\') {\\n                if(document.querySelector(\"select\").value == \\'Open\\') {\\n                    document.querySelector(\\'body\\').innerHTML = html\\n                } else {\\n                    alert(\\'fix the dropdown!!!\\')\\n                }\\n            } else {\\n                alert(\\'write cat in there!!!\\')\\n            }\\n        })\\n    </script>\\n\\n</body></html>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_4 = await page.content()\n",
    "html_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <script>\n",
      "   const html = `<h1>You did it</h1>`\n",
      "  </script>\n",
      " </head>\n",
      " <body>\n",
      "  <div id=\"things\">\n",
      "  </div>\n",
      "  <p>\n",
      "   The secret is\n",
      "   <select>\n",
      "    <option selected=\"\">\n",
      "     Closed\n",
      "    </option>\n",
      "    <option>\n",
      "     Open\n",
      "    </option>\n",
      "   </select>\n",
      "  </p>\n",
      "  <p>\n",
      "   <input id=\"best-animal\" placeholder=\"write cat in here\" type=\"text\"/>\n",
      "  </p>\n",
      "  <p>\n",
      "   <input id=\"submit\" type=\"button\" value=\"Click me\"/>\n",
      "  </p>\n",
      "  <script>\n",
      "   document.querySelector(\"#submit\").addEventListener('click', function() {\n",
      "            if(document.querySelector('#best-animal').value == 'cat') {\n",
      "                if(document.querySelector(\"select\").value == 'Open') {\n",
      "                    document.querySelector('body').innerHTML = html\n",
      "                } else {\n",
      "                    alert('fix the dropdown!!!')\n",
      "                }\n",
      "            } else {\n",
      "                alert('write cat in there!!!')\n",
      "            }\n",
      "        })\n",
      "  </script>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soup_doc_four = BeautifulSoup(html_4)\n",
    "print(soup_doc_four.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object Locator can't be used in 'await' expression",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mlocator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwrite cat in here\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mget_by_label(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mselect_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mget_by_role(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbutton\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClick me\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mclick()\n",
      "\u001b[0;31mTypeError\u001b[0m: object Locator can't be used in 'await' expression"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import time\n",
    "await page.locator(\"write cat in here\")\n",
    "await page.get_by_label(\"Select:\").select_option('Open')\n",
    "await page.get_by_role(\"button\", name=\"Click me\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You did it'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1_content = await page.inner_text('h1')\n",
    "h1_content       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a single table row\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/single-table-row.html, creating a dictionary out of the title, subhead, and byline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutError",
     "evalue": "Page.inner_text: Timeout 30000ms exceeded.\nCall log:\nwaiting for locator(\"td.title\")\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[182], line 29\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# Print the dictionary\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(row_data)\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m scrape_table_row()\n",
      "Cell \u001b[0;32mIn[182], line 14\u001b[0m, in \u001b[0;36mscrape_table_row\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m browser\u001b[38;5;241m.\u001b[39mnew_page()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39mgoto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://jonathansoma.com/columbia/interactive-scrape/single-table-row.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39minner_text(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd.title\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m subhead \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39minner_text(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd.subhead\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m byline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m page\u001b[38;5;241m.\u001b[39minner_text(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd.byline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/playwright/async_api/_generated.py:10712\u001b[0m, in \u001b[0;36mPage.inner_text\u001b[0;34m(self, selector, strict, timeout)\u001b[0m\n\u001b[1;32m  10683\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_text\u001b[39m(\n\u001b[1;32m  10684\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10685\u001b[0m     selector: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10688\u001b[0m     timeout: typing\u001b[38;5;241m.\u001b[39mOptional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m  10689\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m  10690\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Page.inner_text\u001b[39;00m\n\u001b[1;32m  10691\u001b[0m \n\u001b[1;32m  10692\u001b[0m \u001b[38;5;124;03m    Returns `element.innerText`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10708\u001b[0m \u001b[38;5;124;03m    str\u001b[39;00m\n\u001b[1;32m  10709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m  10711\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping\u001b[38;5;241m.\u001b[39mfrom_maybe_impl(\n\u001b[0;32m> 10712\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl_obj\u001b[38;5;241m.\u001b[39minner_text(\n\u001b[1;32m  10713\u001b[0m             selector\u001b[38;5;241m=\u001b[39mselector, strict\u001b[38;5;241m=\u001b[39mstrict, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m  10714\u001b[0m         )\n\u001b[1;32m  10715\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/playwright/_impl/_page.py:955\u001b[0m, in \u001b[0;36mPage.inner_text\u001b[0;34m(self, selector, strict, timeout)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_text\u001b[39m(\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28mself\u001b[39m, selector: \u001b[38;5;28mstr\u001b[39m, strict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    954\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_main_frame\u001b[38;5;241m.\u001b[39minner_text(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlocals_to_params(\u001b[38;5;28mlocals\u001b[39m()))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/playwright/_impl/_frame.py:618\u001b[0m, in \u001b[0;36mFrame.inner_text\u001b[0;34m(self, selector, strict, timeout)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_text\u001b[39m(\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28mself\u001b[39m, selector: \u001b[38;5;28mstr\u001b[39m, strict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    617\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_channel\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minnerText\u001b[39m\u001b[38;5;124m\"\u001b[39m, locals_to_params(\u001b[38;5;28mlocals\u001b[39m()))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/playwright/_impl/_connection.py:59\u001b[0m, in \u001b[0;36mChannel.send\u001b[0;34m(self, method, params)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend\u001b[39m(\u001b[38;5;28mself\u001b[39m, method: \u001b[38;5;28mstr\u001b[39m, params: Dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mwrap_api_call(\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_send(method, params, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/playwright/_impl/_connection.py:520\u001b[0m, in \u001b[0;36mConnection.wrap_api_call\u001b[0;34m(self, cb, is_internal)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m cb()\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m rewrite_error(error, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparsed_st[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapiName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_zone\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Page.inner_text: Timeout 30000ms exceeded.\nCall log:\nwaiting for locator(\"td.title\")\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "async def scrape_table_row():\n",
    "    async with async_playwright() as p:\n",
    "  \n",
    "        browser = await p.chromium.launch(headless=False)  \n",
    "        page = await browser.new_page()\n",
    "\n",
    "      \n",
    "        await page.goto(\"http://jonathansoma.com/columbia/interactive-scrape/single-table-row.html\")\n",
    "\n",
    "    \n",
    "        title = await page.inner_text('td.title')\n",
    "        subhead = await page.inner_text('td.subhead')\n",
    "        byline = await page.inner_text('td.byline')\n",
    "\n",
    "        # Create a dictionary\n",
    "        row_data = {\n",
    "            \"title\": title,\n",
    "            \"subhead\": subhead,\n",
    "            \"byline\": byline\n",
    "        }\n",
    "\n",
    "        # Print the dictionary\n",
    "        print(row_data)\n",
    "\n",
    "\n",
    "await scrape_table_row()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'subhead' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m row_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m: title,\n\u001b[0;32m----> 3\u001b[0m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubhead\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43msubhead\u001b[49m,\n\u001b[1;32m      4\u001b[0m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbyline\u001b[39m\u001b[38;5;124m\"\u001b[39m: byline\n\u001b[1;32m      5\u001b[0m         }\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(row_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'subhead' is not defined"
     ]
    }
   ],
   "source": [
    "row_data = {\n",
    " \"title\": title,\n",
    " \"subhead\": subhead,\n",
    " \"byline\": byline\n",
    "        }\n",
    "\n",
    "print(row_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving into a dictionary\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/single-table-row.html, saving the title, subhead, and byline into a single dictionary called `book`.\n",
    "\n",
    "> Don't use pandas for this one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/single-table-row.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/single-table-row.html' method='GET'>>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do i have to save dictionaries by importing pickle? \n",
    "\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "page = await browser.new_page()\n",
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/single-table-row.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_6 = await page.content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "<html><head><script>\n",
       "    const html = `<table>\n",
       "  <tr>\n",
       "    <td>How to Scrape Things</td>\n",
       "    <td>Probably using Playwright</td>\n",
       "    <td>By Jonathan Soma</td>\n",
       "  </tr>\n",
       "</table>\n",
       "`\n",
       "\n",
       "setTimeout(() => {\n",
       "    console.log(html)\n",
       "    document.querySelector('body').innerHTML = html\n",
       "}, 250)</script>\n",
       "</head><body><table>\n",
       "<tbody><tr>\n",
       "<td>How to Scrape Things</td>\n",
       "<td>Probably using Playwright</td>\n",
       "<td>By Jonathan Soma</td>\n",
       "</tr>\n",
       "</tbody></table>\n",
       "</body></html>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_doc_six = BeautifulSoup(html_6, 'html.parser')\n",
    "soup_doc_six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<td>How to Scrape Things</td>, <td>Probably using Playwright</td>, <td>By Jonathan Soma</td>]\n"
     ]
    }
   ],
   "source": [
    "td_tags=soup_doc_six.find_all('td')\n",
    "print(td_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'How to Scrape Things', 'subhead': 'Probably using Playwright', 'byline': 'By Jonathan Soma'}\n"
     ]
    }
   ],
   "source": [
    "book = {\n",
    "    \"title\": td_tags[0].text.strip(),\n",
    "    \"subhead\": td_tags[1].text.strip(),\n",
    "    \"byline\": td_tags[2].text.strip()\n",
    "}\n",
    "\n",
    "# Print the dictionary\n",
    "print(book)\n",
    "\n",
    "#next step is to save the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to Scrape Things\n",
      "Probably using Playwright\n",
      "By Jonathan Soma\n"
     ]
    }
   ],
   "source": [
    "for td in td_tags:\n",
    "    print(td.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping multiple table rows\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html, creating a list of dictionaries. Convert to a pandas dataframe with `pd.json_normalize`. Save it as `output.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html' method='GET'>>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless=False)\n",
    "page = await browser.new_page()\n",
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = await page.content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "<html><head><script>\n",
       "    const html = `<table>\n",
       "  <tr>\n",
       "    <td>How to Scrape Things</td>\n",
       "    <td>Probably using Playwright</td>\n",
       "    <td>By Jonathan Soma</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>How to Scrape Many Things</td>\n",
       "    <td>But, Is It Even Possible?</td>\n",
       "    <td>By Sonathan Joma</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>The End of Scraping</td>\n",
       "    <td>Let's All Use CSV Files</td>\n",
       "    <td>By Amos Nathanos</td>\n",
       "  </tr>\n",
       "</table>\n",
       "`\n",
       "\n",
       "setTimeout(() => {\n",
       "    document.querySelector('body').innerHTML = html\n",
       "}, 250)</script>\n",
       "</head><body><table>\n",
       "<tbody><tr>\n",
       "<td>How to Scrape Things</td>\n",
       "<td>Probably using Playwright</td>\n",
       "<td>By Jonathan Soma</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>How to Scrape Many Things</td>\n",
       "<td>But, Is It Even Possible?</td>\n",
       "<td>By Sonathan Joma</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>The End of Scraping</td>\n",
       "<td>Let's All Use CSV Files</td>\n",
       "<td>By Amos Nathanos</td>\n",
       "</tr>\n",
       "</tbody></table>\n",
       "</body></html>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_doc = BeautifulSoup(html, 'html.parser')\n",
    "soup_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tr>\n",
       " <td>How to Scrape Things</td>\n",
       " <td>Probably using Playwright</td>\n",
       " <td>By Jonathan Soma</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>How to Scrape Many Things</td>\n",
       " <td>But, Is It Even Possible?</td>\n",
       " <td>By Sonathan Joma</td>\n",
       " </tr>,\n",
       " <tr>\n",
       " <td>The End of Scraping</td>\n",
       " <td>Let's All Use CSV Files</td>\n",
       " <td>By Amos Nathanos</td>\n",
       " </tr>]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_tr=soup_doc.find_all('tr')\n",
    "full_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_tr.find_all('tr')[0].find_all('td')[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'How to Scrape Things',\n",
       " 'subhead': 'Probably using Playwright',\n",
       " 'byline': 'By Jonathan Soma'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info = full_tr[0]\n",
    "dictionary = {}\n",
    "dictionary['title'] = info.find_all('td')[0].text\n",
    "dictionary['subhead'] = info.find_all('td')[1].text\n",
    "dictionary['byline'] = info.find_all('td')[2].text\n",
    "dictionary\n",
    "\n",
    "\n",
    "# for row in full_tr:\n",
    "#     item = {\n",
    "#         \"field1\":row.find_all('tr')[0].find_all('td')[0].text.strip()\n",
    "#         \"field2\":row.find_all('tr')[1].find_all('td')[1].text.strip() \n",
    "#         \"field3\":row.find_all('tr')[0].find_all('td')[2].text.strip() \n",
    "#             }\n",
    "#         data_list.append(item)\n",
    "\n",
    "# data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'How to Scrape Many Things',\n",
       " 'subhead': 'But, Is It Even Possible?',\n",
       " 'byline': 'By Sonathan Joma'}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info1 = full_tr[1]\n",
    "dictionary1 = {}\n",
    "dictionary1['title'] = info1.find_all('td')[0].text\n",
    "dictionary1['subhead'] = info1.find_all('td')[1].text\n",
    "dictionary1['byline'] = info1.find_all('td')[2].text\n",
    "dictionary1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The End of Scraping',\n",
       " 'subhead': \"Let's All Use CSV Files\",\n",
       " 'byline': 'By Amos Nathanos'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info2 = full_tr[2]\n",
    "dictionary2 = {}\n",
    "dictionary2['title'] = info2.find_all('td')[0].text\n",
    "dictionary2['subhead'] = info2.find_all('td')[1].text\n",
    "dictionary2['byline'] = info2.find_all('td')[2].text\n",
    "dictionary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'How to Scrape Many Things',\n",
       "  'subhead': 'But, Is It Even Possible?',\n",
       "  'byline': 'By Sonathan Joma'},\n",
       " {'title': 'How to Scrape Many Things',\n",
       "  'subhead': 'But, Is It Even Possible?',\n",
       "  'byline': 'By Sonathan Joma'},\n",
       " {'title': 'The End of Scraping',\n",
       "  'subhead': \"Let's All Use CSV Files\",\n",
       "  'byline': 'By Amos Nathanos'}]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list=[dictionary, dictionary1, dictionary2]\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subhead</th>\n",
       "      <th>byline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to Scrape Many Things</td>\n",
       "      <td>But, Is It Even Possible?</td>\n",
       "      <td>By Sonathan Joma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to Scrape Many Things</td>\n",
       "      <td>But, Is It Even Possible?</td>\n",
       "      <td>By Sonathan Joma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The End of Scraping</td>\n",
       "      <td>Let's All Use CSV Files</td>\n",
       "      <td>By Amos Nathanos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       title                    subhead            byline\n",
       "0  How to Scrape Many Things  But, Is It Even Possible?  By Sonathan Joma\n",
       "1  How to Scrape Many Things  But, Is It Even Possible?  By Sonathan Joma\n",
       "2        The End of Scraping    Let's All Use CSV Files  By Amos Nathanos"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.json_normalize(new_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to csv for non-data people\n",
    "df.to_csv(\"table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping an actual table\n",
    "\n",
    "Scrape the content at http://jonathansoma.com/columbia/interactive-scrape/the-actual-table.html using pandas' HTML reading function. Save it as `output.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response url='https://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html' request=<Request url='https://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html' method='GET'>>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = await browser.new_page()\n",
    "await page.goto(\"https://jonathansoma.com/columbia/interactive-scrape/multiple-table-rows.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<!DOCTYPE html><html><head><script>\\n    const html = `<table>\\n  <tr>\\n    <td>How to Scrape Things</td>\\n    <td>Probably using Playwright</td>\\n    <td>By Jonathan Soma</td>\\n  </tr>\\n  <tr>\\n    <td>How to Scrape Many Things</td>\\n    <td>But, Is It Even Possible?</td>\\n    <td>By Sonathan Joma</td>\\n  </tr>\\n  <tr>\\n    <td>The End of Scraping</td>\\n    <td>Let's All Use CSV Files</td>\\n    <td>By Amos Nathanos</td>\\n  </tr>\\n</table>\\n`\\n\\nsetTimeout(() => {\\n    document.querySelector('body').innerHTML = html\\n}, 250)</script>\\n</head><body><table>\\n  <tbody><tr>\\n    <td>How to Scrape Things</td>\\n    <td>Probably using Playwright</td>\\n    <td>By Jonathan Soma</td>\\n  </tr>\\n  <tr>\\n    <td>How to Scrape Many Things</td>\\n    <td>But, Is It Even Possible?</td>\\n    <td>By Sonathan Joma</td>\\n  </tr>\\n  <tr>\\n    <td>The End of Scraping</td>\\n    <td>Let's All Use CSV Files</td>\\n    <td>By Amos Nathanos</td>\\n  </tr>\\n</tbody></table>\\n</body></html>\""
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = await page.content()\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "<html><head><script>\n",
       "    const html = `<table>\n",
       "  <tr>\n",
       "    <td>How to Scrape Things</td>\n",
       "    <td>Probably using Playwright</td>\n",
       "    <td>By Jonathan Soma</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>How to Scrape Many Things</td>\n",
       "    <td>But, Is It Even Possible?</td>\n",
       "    <td>By Sonathan Joma</td>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <td>The End of Scraping</td>\n",
       "    <td>Let's All Use CSV Files</td>\n",
       "    <td>By Amos Nathanos</td>\n",
       "  </tr>\n",
       "</table>\n",
       "`\n",
       "\n",
       "setTimeout(() => {\n",
       "    document.querySelector('body').innerHTML = html\n",
       "}, 250)</script>\n",
       "</head><body><table>\n",
       "<tbody><tr>\n",
       "<td>How to Scrape Things</td>\n",
       "<td>Probably using Playwright</td>\n",
       "<td>By Jonathan Soma</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>How to Scrape Many Things</td>\n",
       "<td>But, Is It Even Possible?</td>\n",
       "<td>By Sonathan Joma</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<td>The End of Scraping</td>\n",
       "<td>Let's All Use CSV Files</td>\n",
       "<td>By Amos Nathanos</td>\n",
       "</tr>\n",
       "</tbody></table>\n",
       "</body></html>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_doc = BeautifulSoup(html, 'html.parser')\n",
    "soup_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet html5lib lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How to Scrape Things</td>\n",
       "      <td>Probably using Playwright</td>\n",
       "      <td>By Jonathan Soma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How to Scrape Many Things</td>\n",
       "      <td>But, Is It Even Possible?</td>\n",
       "      <td>By Sonathan Joma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The End of Scraping</td>\n",
       "      <td>Let's All Use CSV Files</td>\n",
       "      <td>By Amos Nathanos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0                          1                 2\n",
       "0       How to Scrape Things  Probably using Playwright  By Jonathan Soma\n",
       "1  How to Scrape Many Things  But, Is It Even Possible?  By Sonathan Joma\n",
       "2        The End of Scraping    Let's All Use CSV Files  By Amos Nathanos"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "tables = pd.read_html(io.StringIO(html))\n",
    "df = tables[0]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to csv for non-data people\n",
    "df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `html.parser` vs `html5lib`\n",
    "\n",
    "Here is some good HTML:\n",
    "\n",
    "```python\n",
    "html_good = \"\"\"\n",
    "<h1>This is a title</h1>\n",
    "<h2>This is a subhead</h2>\n",
    "<p>This is a paragraph</p>\n",
    "<p>This is another paragraph</p>\n",
    "\"\"\"\n",
    "\n",
    "Here is some bad HTML:\n",
    "    \n",
    "html_bad = \"\"\"\n",
    "<h1>This is a title\n",
    "<h2>This is a subhead\n",
    "<p>This is a paragraph\n",
    "<p>This is another paragraph\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "When you're using BeautifulSoup, you can use different parsers, including `html.parser`, `html5lib` and `lxml`. Try both the good HTML and bad HTML with each parser and use `print(soup_doc.prettify())` to view the difference.\n",
    "\n",
    "What is different about each one?\n",
    "\n",
    "> You'll need to `pip install` for both html5lib and lxml. Since you aren't important them, they're coming from BeautifulSoup, you'll need to do **Kernel > Restart** and run from the top after installing to have them work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /Users/ananya/.pyenv/versions/3.11.8/lib/python3.11/site-packages (4.9.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: html5lib in /Users/ananya/.pyenv/versions/3.11.8/lib/python3.11/site-packages (1.1)\n",
      "Requirement already satisfied: six>=1.9 in /Users/ananya/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/ananya/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from html5lib) (0.5.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install html5lib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_doc = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parser: html.parser\n",
      "Good HTML:\n",
      "<h1>\n",
      " This is a title\n",
      "</h1>\n",
      "<h2>\n",
      " This is a subhead\n",
      "</h2>\n",
      "<p>\n",
      " This is a paragraph\n",
      "</p>\n",
      "<p>\n",
      " This is another paragraph\n",
      "</p>\n",
      "\n",
      "Bad HTML:\n",
      "<h1>\n",
      " This is a title\n",
      " <h2>\n",
      "  This is a subhead\n",
      "  <p>\n",
      "   This is a paragraph\n",
      "   <p>\n",
      "    This is another paragraph\n",
      "   </p>\n",
      "  </p>\n",
      " </h2>\n",
      "</h1>\n",
      "\n",
      "Using parser: html5lib\n"
     ]
    },
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[181], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parser \u001b[38;5;129;01min\u001b[39;00m parsers:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing parser:\u001b[39m\u001b[38;5;124m\"\u001b[39m, parser)\n\u001b[0;32m---> 18\u001b[0m     soup_good \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml_good\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGood HTML:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(soup_good\u001b[38;5;241m.\u001b[39mprettify())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.8/lib/python3.11/site-packages/bs4/__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# Good HTML\n",
    "html_good = \"\"\"\n",
    "<h1>This is a title</h1>\n",
    "<h2>This is a subhead</h2>\n",
    "<p>This is a paragraph</p>\n",
    "<p>This is another paragraph</p>\n",
    "\"\"\"\n",
    "# Bad HTML\n",
    "html_bad = \"\"\"\n",
    "<h1>This is a title\n",
    "<h2>This is a subhead\n",
    "<p>This is a paragraph\n",
    "<p>This is another paragraph\n",
    "\"\"\"\n",
    "parsers = ['html.parser', 'html5lib', 'lxml']\n",
    "for parser in parsers:\n",
    "    print(\"Using parser:\", parser)\n",
    "    soup_good = BeautifulSoup(html_good, parser)\n",
    "    print(\"Good HTML:\")\n",
    "    print(soup_good.prettify())\n",
    "    soup_bad = BeautifulSoup(html_bad, parser)\n",
    "    print(\"Bad HTML:\")\n",
    "    print(soup_bad.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
